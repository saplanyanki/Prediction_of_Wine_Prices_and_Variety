# -*- coding: utf-8 -*-
"""Gia_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h1X5Grzm0rNT1zykQA7uiITcMyh19Vwy
"""

#Libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, GridSearchCV 
import scipy
import math
import seaborn as sns
from sklearn import metrics
from numpy import mean
from numpy import std
from sklearn.model_selection import cross_val_score
from scipy.stats import mode
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.model_selection import LeaveOneOut

from sklearn.svm import SVR
from sklearn.linear_model import Lasso
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from sklearn.ensemble import StackingRegressor


from google.colab import drive
drive.mount('/content/gdrive')

"""**Prep data**"""

data = pd.read_csv("/content/gdrive/My Drive/DS320_Project/Data/winemag-data_first150k.csv")
data.head(5)

def get_mode_or_x(series):
  mode = series.mode()
  if mode.size == 1:
      return mode[0]
  return "x"

def domestic(row):
    if row['country'] == 'US':
        val = 1
    else:
        val = 0
    return val

def expensive(row):
    means = data.price.mean()
    if row['price'] > means:
        val = 1
    else:
        val = 0
    return val
pd.options.mode.chained_assignment = None  # default='warn'

def titan(data):
  # Remove 'Unnamed: 0' and description
  new_data = data
  new_data.drop('Unnamed: 0', axis=1, inplace=True)
  new_data.drop('description', axis=1, inplace=True)
  new_data = new_data.dropna(subset=['country'])
  new_data.drop('designation', axis=1, inplace=True)
  #data.drop('region_1', axis=1, inplace=True)
  new_data.drop('region_2', axis=1, inplace=True)

  # Replace with variety mean
  new_data['price'].fillna(new_data.groupby(['variety'])['price'].transform('mean'), inplace=True) # Leaves some left over cases with na
  print('======VARIETY MEAN NA======')
  new_data = new_data.dropna(subset=['price']) # Removes left over cases
  print('======FINISHED DROPPING======')
  #new_data.loc[:, "region_1"] = new_data['region_1'].fillna(data.groupby("province")["region_1"].transform(get_mode_or_x))
  #new_data = new_data[data.region_1 != 'x']
  new_data.drop('region_1', axis=1, inplace=True)
  print('======MODE NA FOR REGION_1======')
  #new_data['Domestic'] = new_data.apply(domestic, axis=1)
  #new_data.drop('country', axis=1, inplace=True)
  new_data['Expensive'] = new_data.apply(expensive, axis=1)
  for col in ['variety', 'province', 'winery']:
    new_data[col] = new_data[col].str.lower()
  print('======STRING LOW FOR ALL======')

  ##Reduce the lower counts from columns to avoid high dimensionality
  counts = new_data['variety'].value_counts()
  var_mean = new_data['variety'].value_counts().mean()
  counts1 = new_data['winery'].value_counts()
  winery_mean = new_data['winery'].value_counts().mean()
  new_data = new_data[~new_data['variety'].isin(counts[counts < var_mean].index)]
  new_data = new_data[~new_data['winery'].isin(counts1[counts1 < winery_mean].index)]

  print(len(new_data))
  print('======CLASS BALANCE FOR DOMESTIC======')
  #print('{}'.format(round(new_data['Domestic'].value_counts()/len(new_data)*100,3)))
  #print('{}'.format(round(new_data['variety'].value_counts()/len(new_data)*100,3)))
  #print('{}'.format(round(new_data['winery'].value_counts()/len(new_data)*100,3)))
  print('======CLASS BALANCE FOR VARIETY======')
  print('======THANKS FOR USING TITAN======')
  return new_data


newdata = titan(data)

counts1 = newdata['winery'].value_counts()
winery_mean = newdata['winery'].value_counts().mean()
newdata = newdata[~newdata['winery'].isin(counts1[counts1 < winery_mean].index)]
newdata.winery.value_counts()
a = newdata

b = pd.DataFrame(a.groupby('variety').count()['Expensive'] / len(a)).reset_index()
b = b.rename(columns={'Expensive': 'Posterior_variety_expensive'})
newdata1 = pd.merge(a, b, how ='left', on ='variety')
newdata1

cat_vars = ['country', 'province', 'variety', 'winery']
newdata1[cat_vars] = newdata1[cat_vars].astype(str)
newdata1.info()

newdata1.describe()

newdata1 = newdata1.groupby(['country']).agg(
    median_price=('price', 'median'),
    max_price=('price', 'max'),
    min_price=('price', 'min'),
    mean_points=('points', 'mean'),
    most_frequent_province=('province', lambda x: mode(x)[0][0]),
    most_frequent_variety=('variety', lambda x: mode(x)[0][0]),
    most_frequent_winery=('winery', lambda x: mode(x)[0][0])
)
newdata1

cat_vars = ['most_frequent_province',
            'most_frequent_variety',
            'most_frequent_winery']
newdata1[cat_vars] = newdata1[cat_vars].astype(str)
newdata1 = pd.get_dummies(newdata1)

newdata1

y = newdata1['mean_points'].values
X = newdata1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 77)

"""**PCA**"""

# Standardizing
pipeline = Pipeline([
    ('std_scalar', StandardScaler())
])

X_train = pipeline.fit_transform(X_train)
X_test = pipeline.transform(X_test)

# Loop Function to identify number of principal components that explain at least 90% of the variance
for comp in range(3, X_train.shape[1]):
    pca = PCA(n_components= comp, random_state=42)
    pca.fit(X_train)
    comp_check = pca.explained_variance_ratio_
    final_comp = comp
    if comp_check.sum() > 0.90:
        break
        
Final_PCA = PCA(n_components= final_comp,random_state=42)
Final_PCA.fit(X_train)
cluster_df=Final_PCA.transform(X_test)
num_comps = comp_check.shape[0]
print("Using {} components, we can explain {}% of the variability in the original data.".format(final_comp,comp_check.sum()))

figure = plt.figure()
ax = plt.gca()
plt.plot(pca.explained_variance_, color='red', linestyle='dotted')
ax.set_title("Scree plot")
ax.set_xlabel("Index of principal components")
ax.set_ylabel("The explained varaince")

"""**Modeling**"""

def cross_val(model):
    pred = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=LeaveOneOut(),n_jobs=-1)
    return pred.mean()

def print_evaluate(true, predicted):  
    mae = metrics.mean_absolute_error(true, predicted)
    mse = metrics.mean_squared_error(true, predicted)
    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))
    r2_square = metrics.r2_score(true, predicted)
    print('MAE:', mae)
    print('MSE:', mse)
    print('RMSE:', rmse)
    print('R2 Square', r2_square)
    print('__________________________________')
    
def evaluate(true, predicted):
    mae = metrics.mean_absolute_error(true, predicted)
    mse = metrics.mean_squared_error(true, predicted)
    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))
    r2_square = metrics.r2_score(true, predicted)
    return mae, mse, rmse, r2_square

#Linear
lin_reg = make_pipeline(StandardScaler(with_mean=False), LinearRegression())
lin_reg.fit(X_train,y_train)

test_pred = lin_reg.predict(X_test)
train_pred = lin_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df = pd.DataFrame(data=[["Linear Regression", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], 
                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])

#SVM
svm_reg = SVR(kernel='rbf', C=10, epsilon=0.001)
svm_reg.fit(X_train, y_train)

test_pred = svm_reg.predict(X_test)
train_pred = svm_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)

print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["SVM Regressor", *evaluate(y_test, test_pred), 0]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)

#Random Forest
rf_reg = RandomForestRegressor(n_estimators=100)
rf_reg.fit(X_train, y_train)

test_pred = rf_reg.predict(X_test)
train_pred = rf_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)

print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["Random Forest Regressor", *evaluate(y_test, test_pred), 0]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])
results_df = results_df.append(results_df_2, ignore_index=True)

#Ridge
ridge_reg = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)
ridge_reg.fit(X_train, y_train)
pred = ridge_reg.predict(X_test)

test_pred = ridge_reg.predict(X_test)
train_pred = ridge_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["Ridge Regression", *evaluate(y_test, test_pred) , cross_val(Ridge())]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])
results_df = results_df.append(results_df_2, ignore_index=True)

#Lasso
lasso_reg = Lasso(alpha=0.1, 
              precompute=True, 
#               warm_start=True, 
              positive=True, 
              selection='random',
              random_state=42)
lasso_reg.fit(X_train, y_train)

test_pred = lasso_reg.predict(X_test)
train_pred = lasso_reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["Lasso Regression", *evaluate(y_test, test_pred) , cross_val(Lasso())]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])
results_df = results_df.append(results_df_2, ignore_index=True)

"""**Ensemble Learning**"""

all_models = [
    ('svr', svm_reg), 
    ('rf', rf_reg),
    ('ridge', ridge_reg),
    ('lasso', lasso_reg),
    ('linear', lin_reg)
    ]

#all models
reg = StackingRegressor(estimators = all_models, final_estimator = RandomForestRegressor())

reg.fit(X_train, y_train)
pred = reg.predict(X_test)

test_pred = reg.predict(X_test)
train_pred = reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["Ensemble Stacking All Models", *evaluate(y_test, test_pred) , 0]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])
results_df = results_df.append(results_df_2, ignore_index=True)

#Top 2 performing algorithms (Random Forest & Lasso)
top_algo = [
    ('lasso', lasso_reg),
    ('rf', rf_reg)
            ]
reg = StackingRegressor(estimators = top_algo, final_estimator = RandomForestRegressor())

reg.fit(X_train, y_train)
pred = reg.predict(X_test)

test_pred = reg.predict(X_test)
train_pred = reg.predict(X_train)

print('Test set evaluation:\n_____________________________________')
print_evaluate(y_test, test_pred)
print('====================================')
print('Train set evaluation:\n_____________________________________')
print_evaluate(y_train, train_pred)

results_df_2 = pd.DataFrame(data=[["Ensemble Stacking RF & Lasso", *evaluate(y_test, test_pred) , 0]], 
                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])
results_df = results_df.append(results_df_2, ignore_index=True)

"""**Model Comparison**"""

results_df1 = results_df.sort_values(['RMSE'], ascending=[1])
results_df1

results_df = results_df.reset_index(drop=False)
results_df.set_index('Model', inplace=True)
results_df.sort_values('R2 Square', inplace=True)
results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))

results_df.to_csv('cluster_results.csv')